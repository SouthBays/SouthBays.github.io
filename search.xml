<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>三十而立</title>
      <link href="/2022/02/26/san-shi-er-li/"/>
      <url>/2022/02/26/san-shi-er-li/</url>
      
        <content type="html"><![CDATA[<p>如果有一种人生叫成功，那一定是能按照自己的计划去铺展未来。</p><p>我的人生，就是能够选择自己擅长的事情，一步步积累成就感。</p><blockquote><p>『无题』 <sub><sup>风の诗</sup></sub><br>人闻醉酒多诗仙，<br>唯有醒者真圣贤。<br>梦里有之不必有，<br>梦里无之倏忽见。<br>白驹过隙一页黄，<br>沧海闻涛往事烟。<br>此去灵山多有路，<br>独不见不知所然。  </p></blockquote><p>诗与酒，酒是麻痹神经的，诗可以遨游太虚而不需要任何代价，何不如清醒地诗意人生？</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
          <category> 诗歌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP对比学习论文笔记</title>
      <link href="/2022/02/10/nlp-dui-bi-xue-xi-lun-wen-xue-xi/"/>
      <url>/2022/02/10/nlp-dui-bi-xue-xi-lun-wen-xue-xi/</url>
      
        <content type="html"><![CDATA[<h1 id="一、预训练-对比学习-SOTA句向量？"><a href="#一、预训练-对比学习-SOTA句向量？" class="headerlink" title="一、预训练+对比学习=SOTA句向量？"></a>一、预训练+对比学习=SOTA句向量？</h1><p>===<br><a href="https://arxiv.org/pdf/2104.08821.pdf" target="_blank" rel="noopener">(PaperOrigin)</a>  <a href="./对比学习/2104.08821.pdf">(PaperLocal)</a>  </p><ol><li>Contracdiction pairs of NLI(自然语言推理)<br> 先看nli推理是将句子对输出两次句向量，并计算其相似度。<br> <img src="/resources/nlinet.png" alt>  </li></ol><ol start="2"><li><p>Pretraining model(预训练模型)<br> 再看一个利用bert的[CLS]标签编码两两作点积来训练相似度(主要看左侧矩阵，来自苏剑林的simbert)。<br> <img src="/resources/simbert.png" alt>  </p><blockquote><p>第一个方法是构造相似或者不向似的句子对，预测0和1的标签，是一个有监督学习<br>第二个方法可以用于无监督学习。在一个batch的n个句子中，自己作为正例，除了自己以外的其它句子都作为负例，最后生成一个n<em>n的相似度矩阵，预测的目标就是一个n</em>n的单位矩阵  </p></blockquote><p> SimCSE就是融合了两种方法，它的正例是对自己重复编码dropout生成的不同向量(后面会具体解释)，负例是是其它句子重复编码生成的向量，同样生成一个n*n的相似度矩阵。<br> <img src="/resources/simcse.jpg" alt>  </p></li></ol><h1 id="二、原理"><a href="#二、原理" class="headerlink" title="二、原理"></a>二、原理</h1><ol><li><p>info NCE loss<br><img src="/resources/infonce_loss.jpg" alt>  </p></li><li><p>stand dropout vs other data augmentation</p><ul><li><code>SimCSE (dropout)</code> NLP is interesting. —— NLP is interesting.</li><li><code>Next sentence</code> I do NLP. —— NLP is interesting.  </li><li><code>Synonym replacement</code> The movie is <u>great</u>. —— The movie is <u>fantastic</u>.  </li><li><code>crops</code> <del>Two dogs</del> are running. —— <del>Two</del> dogs are <del>running</del>.  </li><li><code>Delete one word</code> Two dogs are <del>running</del>. —— Two dogs <del>are</del> running.</li></ul></li></ol><h1 id="三、深层原因"><a href="#三、深层原因" class="headerlink" title="三、深层原因?"></a>三、深层原因?</h1><ol><li><p><strong>alignment</strong> vs. <strong>uniformity</strong><a href="https://arxiv.org/pdf/2005.10242.pdf" target="_blank" rel="noopener">(PaperOrigin)</a>  <a href="./对比学习/2005.10242.pdf">(PaperLocal)</a><br> <img src="/resources/alignment_uniformity.jpg" alt>   </p><blockquote><p>alignment:正样本的对齐性<br>uniformity:所有样本分布的一致性  </p></blockquote></li><li><p>对SimCSE等方法定量分析训练过程中的对齐性和一致性<br> <img src="/resources/simcse_deepdive.jpg" alt>  </p><blockquote><p>SimCSE表现最好，在提升一致性的同时很好的保持了对齐性  </p></blockquote></li></ol><h1 id="四、思考："><a href="#四、思考：" class="headerlink" title="四、思考："></a>四、思考：</h1><ol><li>bert的原始的[CLS]向量不可靠  </li><li>前置研究句向量的降维-bert whitening也印证了[CLS]向量包含较多的冗余信息，当然，whitening最主要的作用还是将句向量转化到标准正交基上，从而更好地使用余弦相似度<br> <a href="https://arxiv.org/abs/2103.15316" target="_blank" rel="noopener">(PaperOrigin)</a>  <a href="./对比学习/2103.15316.pdf">(PaperLocal)</a><br> <a href="https://spaces.ac.cn/archives/8069" target="_blank" rel="noopener">博客地址</a><br> 一个线性变换-PCA降维可以把维度从768降到256,1024降到384而保持差不多的结果。  </li><li>SimCSE可以处理的文本更长，因为句子对是分别编码的，而降维思想可以在储存和检索向量时大大减少空间和内存的占用从而提升检索效率  <ul><li>应用场景1. 实体链接  </li><li>应用场景2. 文本聚类  </li><li>应用场景3. 问句检索  </li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 论文学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法拾贝 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python搭起鹊桥！cuda和tensorrt终相会</title>
      <link href="/2021/08/22/tensorrt-bu-shu-shi-zhan/"/>
      <url>/2021/08/22/tensorrt-bu-shu-shi-zhan/</url>
      
        <content type="html"><![CDATA[<p>经过几天的奋战，终于完成tensorrt的官方文档的梳理并实现了基于pycuda的后端部署，</p><p>因为python相关资料确实少，所以想分享一下自己的经验。首先总结一下tensorrt的三个特性</p><ul><li>速度就是正义：GPU相对加速50%以上。半精度和int8加持可以翻倍，官方案例TensorRT 8，BERT-Large推理仅需1.2毫秒！（只支持GPU，因为运行需要特定的算子kenel，被一些文章误导了很久，笔者有误必勘，欢迎大家指正）  </li><li>通用性：支持多深度学习框架，目前最流行torch-&gt;onnx-&gt;tensorRT，tf原生支持（pb-&gt;tf-trt，如果trt没有的算子就会用tf自带的算子替代，速度比纯trt要慢一些） </li><li>定制性：tensorrt根据不同显卡定制优化引擎,同等计算能力才能勉强兼容，而且同一型号显卡如果显存和gpu核心时钟频率不一致也会对性能造成影响。tf2onnx</li></ul><h1 id="一、tensorrt-workflow"><a href="#一、tensorrt-workflow" class="headerlink" title="一、tensorrt workflow"></a>一、tensorrt workflow</h1><p>tensorrt runtime api的核心是trt engine和trt context（这里和cuda context管理区分开）  </p><ul><li><code>官方工作流程</code><br><img src="/resources/official-workflow.png" alt></li><li><code>实战编码流程</code>:Builder-&gt;Network-&gt;Parser/Self define-&gt;<code>Engine</code>-&gt;<code>Context</code>-&gt;Cuda coding</li></ul><h1 id="二、tensorrt-7-Dynamic-Shapes"><a href="#二、tensorrt-7-Dynamic-Shapes" class="headerlink" title="二、tensorrt(7+) Dynamic Shapes"></a>二、tensorrt(7+) Dynamic Shapes</h1><p>6以上支持batch，7以上支持<code>batch和动态尺寸</code>，8以上支持更多onnx算子(如gather elements)，这点和onnx一样，<code>向下兼容性</code>做的比较好</p><ol><li>explicit batch dimension  <ul><li>Python  <pre class="line-numbers language-python"><code class="language-python">create_network<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">&lt;&lt;</span>int<span class="token punctuation">(</span>tensorrt<span class="token punctuation">.</span>NetworkDefinitionCreationFlag<span class="token punctuation">.</span>EXPLICIT_BATCH<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul></li><li><a href="#runtime-dimension">runtime dimension</a> 动态的维度用-1表示。从onnx解析的network会自动推测动态的维度。  </li><li><a href="#buildtime-dimension">buildtime dimension</a> build engine时指定optimization profiles来设置动态tensor形状的范围  </li><li><a href="#use-engine">use engine</a><br> a. Create an execution context from the engine, the same as without dynamic shapes.<br> b. Specify one of the optimization profiles from step 3 that covers the input dimensions.<br> c. Specify the input dimensions for the execution context.<br> d. Enqueue work.  </li></ol><h2 id="runtime-dimension"><a href="#runtime-dimension" class="headerlink" title="runtime dimension"></a>runtime dimension</h2><ul><li>if define a network from scrach,input and output shapes should be specified  </li><li>if network is parsed from onnx or other,shapes can be infered automatically  </li><li>python<pre class="line-numbers language-python"><code class="language-python">  <span class="token comment" spellcheck="true"># when defining a network yourself</span>  network_definition<span class="token punctuation">.</span>add_input<span class="token punctuation">(</span><span class="token string">"foo"</span><span class="token punctuation">,</span> trt<span class="token punctuation">.</span>float32<span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># at runtime</span>  context<span class="token punctuation">.</span>set_binding_shape<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">150</span><span class="token punctuation">,</span> <span class="token number">250</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  engine<span class="token punctuation">.</span>get_binding_shape<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># returns (3, -1, -1).</span>  context<span class="token punctuation">.</span>get_binding_shape<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># returns (3, 150, 250).</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h2 id="buildtime-dimension"><a href="#buildtime-dimension" class="headerlink" title="buildtime dimension"></a>buildtime dimension</h2><ul><li>python  <pre class="line-numbers language-python"><code class="language-python">  <span class="token comment" spellcheck="true"># at least one profile should be specified</span>  <span class="token comment" spellcheck="true"># minimum size of [3,100,200], a optimization size of [3,200,300], and maximum dimensions of [3,150,250]</span>  profile <span class="token operator">=</span> builder<span class="token punctuation">.</span>create_optimization_profile<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  profile<span class="token punctuation">.</span>set_shape<span class="token punctuation">(</span><span class="token string">"foo"</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">150</span><span class="token punctuation">,</span> <span class="token number">250</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">200</span><span class="token punctuation">,</span> <span class="token number">300</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   config<span class="token punctuation">.</span>add_optimization_profile<span class="token punctuation">(</span>profile<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><h1 id="三、engine-and-context"><a href="#三、engine-and-context" class="headerlink" title="三、engine and context"></a>三、engine and context</h1><h2 id="docker环境配置-cuda10-2-python3-6-trt8"><a href="#docker环境配置-cuda10-2-python3-6-trt8" class="headerlink" title="docker环境配置 cuda10.2-python3.6-trt8"></a>docker环境配置 cuda10.2-python3.6-trt8</h2><pre class="line-numbers language-shell"><code class="language-shell">FROM nvidia/cuda:10.2-cudnn8-devel-ubuntu18.04ENV TRT_VERSION=8.0.1.6 \TRT_HOME=/usr/local/Tensorrt \CUDA_HOME=/usr/local/cuda \LIBRARY_PATH=/usr/local/cuda/lib64 \LD_LIBRARY_PATH=/usr/local/Tensorrt/lib:/usr/local/cuda/lib64# 需要下载tensorrt对应版本的压缩包解压到当前目录COPY . /usr/local/Tensorrt-$TRT_VERSION/RUN ln -s /usr/local/Tensorrt-$TRT_VERSION /usr/local/Tensorrt \&& pip install $TRT_HOME/python/tensorrt-$TRT_VERSION-cp36-none-linux_x86_64.whl# start# docker run -it --gpus all cuda10.2-python3.6-trt8 bash<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="trt-engine"><a href="#trt-engine" class="headerlink" title="trt engine"></a>trt engine</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">build_or_create_engine</span><span class="token punctuation">(</span>onnx_path<span class="token punctuation">,</span> trt_path<span class="token punctuation">,</span> input_optimitions<span class="token operator">=</span>None<span class="token punctuation">,</span> max_batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> save_engine<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>                           re_build<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    TRT_LOGGER <span class="token operator">=</span> trt<span class="token punctuation">.</span>Logger<span class="token punctuation">(</span><span class="token punctuation">)</span>    EXPLICIT_BATCH <span class="token operator">=</span> <span class="token number">1</span>    <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>trt_path<span class="token punctuation">)</span> <span class="token operator">and</span> <span class="token operator">not</span> re_build<span class="token punctuation">:</span>        <span class="token keyword">with</span> trt<span class="token punctuation">.</span>Runtime<span class="token punctuation">(</span>TRT_LOGGER<span class="token punctuation">)</span> <span class="token keyword">as</span> runtime<span class="token punctuation">,</span> open<span class="token punctuation">(</span>trt_path<span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>            engine <span class="token operator">=</span> runtime<span class="token punctuation">.</span>deserialize_cuda_engine<span class="token punctuation">(</span>f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'deserialized trt engine from file'</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> engine    <span class="token keyword">with</span> trt<span class="token punctuation">.</span>Builder<span class="token punctuation">(</span>TRT_LOGGER<span class="token punctuation">)</span> <span class="token keyword">as</span> builder<span class="token punctuation">,</span> builder<span class="token punctuation">.</span>create_network<span class="token punctuation">(</span>EXPLICIT_BATCH<span class="token punctuation">)</span> <span class="token keyword">as</span> network<span class="token punctuation">,</span> \            builder<span class="token punctuation">.</span>create_builder_config<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> config<span class="token punctuation">:</span>        <span class="token keyword">if</span> <span class="token operator">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>onnx_path<span class="token punctuation">)</span><span class="token punctuation">:</span>            quit<span class="token punctuation">(</span><span class="token string">'onnx file {} not found'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>onnx_path<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">with</span> trt<span class="token punctuation">.</span>OnnxParser<span class="token punctuation">(</span>network<span class="token punctuation">,</span> TRT_LOGGER<span class="token punctuation">)</span> <span class="token keyword">as</span> parser<span class="token punctuation">,</span> open<span class="token punctuation">(</span>onnx_path<span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> model<span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'parsing onnx file {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>onnx_path<span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> <span class="token operator">not</span> parser<span class="token punctuation">.</span>parse<span class="token punctuation">(</span>model<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'ERROR: Failed to parse the ONNX file.'</span><span class="token punctuation">)</span>                <span class="token keyword">for</span> error <span class="token keyword">in</span> range<span class="token punctuation">(</span>parser<span class="token punctuation">.</span>num_errors<span class="token punctuation">)</span><span class="token punctuation">:</span>                    <span class="token keyword">print</span><span class="token punctuation">(</span>parser<span class="token punctuation">.</span>get_error<span class="token punctuation">(</span>error<span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token keyword">return</span>        builder<span class="token punctuation">.</span>max_batch_size <span class="token operator">=</span> max_batch_size        config<span class="token punctuation">.</span>max_workspace_size <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">&lt;&lt;</span> <span class="token number">30</span>        profile <span class="token operator">=</span> builder<span class="token punctuation">.</span>create_optimization_profile<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">[</span>profile<span class="token punctuation">.</span>set_shape<span class="token punctuation">(</span><span class="token operator">*</span>s<span class="token punctuation">)</span> <span class="token keyword">for</span> s <span class="token keyword">in</span> input_optimitions<span class="token punctuation">]</span>        config<span class="token punctuation">.</span>add_optimization_profile<span class="token punctuation">(</span>profile<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'building an engine from file {}; this may take a while...'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>onnx_path<span class="token punctuation">)</span><span class="token punctuation">)</span>        engine <span class="token operator">=</span> builder<span class="token punctuation">.</span>build_engine<span class="token punctuation">(</span>network<span class="token punctuation">,</span> config<span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token operator">not</span> engine<span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"tensorRT engine built failed"</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"tensorRT engine built success"</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> save_engine<span class="token punctuation">:</span>                <span class="token keyword">with</span> open<span class="token punctuation">(</span>trt_path<span class="token punctuation">,</span> <span class="token string">"wb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>                    f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>engine<span class="token punctuation">.</span>serialize<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'tensorRT engine saved'</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> engine<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="trt-context"><a href="#trt-context" class="headerlink" title="trt context"></a>trt context</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">allocate_buffers_and_inference</span><span class="token punctuation">(</span>context<span class="token punctuation">,</span> engine<span class="token punctuation">,</span> trt_inputs<span class="token punctuation">:</span> list <span class="token operator">=</span> None<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    allocate cpu and gpu buffer of inputs and outputs,    init bindings of gpu memory and cuda stream    :param context: trt context    :param engine: trt engine    :param trt_inputs: a list of real input, which is type of numpy array    :return: inputs, outputs, bindings, stream    """</span>    <span class="token keyword">if</span> trt_inputs <span class="token operator">and</span> trt_inputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        real_size <span class="token operator">=</span> min<span class="token punctuation">(</span>engine<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span> len<span class="token punctuation">(</span>trt_inputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        trt_inputs <span class="token operator">=</span> trt_inputs<span class="token punctuation">[</span><span class="token punctuation">:</span> real_size<span class="token punctuation">]</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        real_size <span class="token operator">=</span> engine<span class="token punctuation">.</span>max_batch_size    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'batch size %d'</span> <span class="token operator">%</span> real_size<span class="token punctuation">)</span>    inputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    bindings <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> binding <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>engine<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>binding<span class="token punctuation">,</span> engine<span class="token punctuation">.</span>get_binding_shape<span class="token punctuation">(</span>binding<span class="token punctuation">)</span><span class="token punctuation">)</span>        size <span class="token operator">=</span> abs<span class="token punctuation">(</span>trt<span class="token punctuation">.</span>volume<span class="token punctuation">(</span>engine<span class="token punctuation">.</span>get_binding_shape<span class="token punctuation">(</span>binding<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> real_size        dtype <span class="token operator">=</span> trt<span class="token punctuation">.</span>nptype<span class="token punctuation">(</span>engine<span class="token punctuation">.</span>get_binding_dtype<span class="token punctuation">(</span>binding<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Allocate host and device buffers</span>        host_mem <span class="token operator">=</span> cuda<span class="token punctuation">.</span>pagelocked_empty<span class="token punctuation">(</span>size<span class="token punctuation">,</span> dtype<span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token operator">not</span> trt_inputs <span class="token keyword">else</span> trt_inputs<span class="token punctuation">[</span>i<span class="token punctuation">]</span>        device_mem <span class="token operator">=</span> cuda<span class="token punctuation">.</span>mem_alloc<span class="token punctuation">(</span>host_mem<span class="token punctuation">.</span>nbytes<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Append the device buffer to device bindings.</span>        bindings<span class="token punctuation">.</span>append<span class="token punctuation">(</span>int<span class="token punctuation">(</span>device_mem<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Append to the appropriate list.</span>        shape <span class="token operator">=</span> <span class="token punctuation">(</span>real_size<span class="token punctuation">,</span> <span class="token operator">*</span>engine<span class="token punctuation">.</span>get_binding_shape<span class="token punctuation">(</span>binding<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">if</span> <span class="token operator">not</span> trt_inputs <span class="token keyword">else</span> trt_inputs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>shape        <span class="token keyword">if</span> engine<span class="token punctuation">.</span>binding_is_input<span class="token punctuation">(</span>binding<span class="token punctuation">)</span><span class="token punctuation">:</span>            inputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'host'</span><span class="token punctuation">:</span> host_mem<span class="token punctuation">,</span> <span class="token string">'device'</span><span class="token punctuation">:</span> device_mem<span class="token punctuation">,</span> <span class="token string">'shape'</span><span class="token punctuation">:</span> shape<span class="token punctuation">}</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'host'</span><span class="token punctuation">:</span> host_mem<span class="token punctuation">,</span> <span class="token string">'device'</span><span class="token punctuation">:</span> device_mem<span class="token punctuation">,</span> <span class="token string">'shape'</span><span class="token punctuation">:</span> shape<span class="token punctuation">}</span><span class="token punctuation">)</span>    stream <span class="token operator">=</span> cuda<span class="token punctuation">.</span>Stream<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        context<span class="token punctuation">.</span>set_binding_shape<span class="token punctuation">(</span>i<span class="token punctuation">,</span> inputs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'shape'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token punctuation">[</span>cuda<span class="token punctuation">.</span>memcpy_htod_async<span class="token punctuation">(</span>inp<span class="token punctuation">[</span><span class="token string">'device'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> inp<span class="token punctuation">[</span><span class="token string">'host'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> stream<span class="token punctuation">)</span> <span class="token keyword">for</span> inp <span class="token keyword">in</span> inputs<span class="token punctuation">]</span>    context<span class="token punctuation">.</span>execute_async_v2<span class="token punctuation">(</span>bindings<span class="token operator">=</span>bindings<span class="token punctuation">,</span> stream_handle<span class="token operator">=</span>stream<span class="token punctuation">.</span>handle<span class="token punctuation">)</span>    <span class="token punctuation">[</span>cuda<span class="token punctuation">.</span>memcpy_dtoh_async<span class="token punctuation">(</span>out<span class="token punctuation">[</span><span class="token string">'host'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> out<span class="token punctuation">[</span><span class="token string">'device'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> stream<span class="token punctuation">)</span> <span class="token keyword">for</span> out <span class="token keyword">in</span> outputs<span class="token punctuation">]</span>    stream<span class="token punctuation">.</span>synchronize<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>outputs<span class="token punctuation">)</span>    <span class="token keyword">return</span> real_size<span class="token punctuation">,</span> <span class="token punctuation">[</span>out<span class="token punctuation">[</span><span class="token string">'host'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> out <span class="token keyword">in</span> outputs<span class="token punctuation">]</span><span class="token keyword">with</span> engine<span class="token punctuation">.</span>create_execution_context<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> context<span class="token punctuation">:</span>    t <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>    batch_size<span class="token punctuation">,</span> trt_outputs <span class="token operator">=</span> allocate_buffers_and_inference<span class="token punctuation">(</span>context<span class="token punctuation">,</span> engine<span class="token punctuation">,</span> trt_inputs<span class="token operator">=</span>None<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'trt infernce average %dms'</span> <span class="token operator">%</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> t<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">1000</span> <span class="token operator">/</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="四、pycuda编程和django-uwsgi部署"><a href="#四、pycuda编程和django-uwsgi部署" class="headerlink" title="四、pycuda编程和django uwsgi部署"></a>四、pycuda编程和django uwsgi部署</h1><pre class="line-numbers language-python"><code class="language-python">    stream <span class="token operator">=</span> cuda<span class="token punctuation">.</span>Stream<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        context<span class="token punctuation">.</span>set_binding_shape<span class="token punctuation">(</span>i<span class="token punctuation">,</span> inputs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'shape'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token punctuation">[</span>cuda<span class="token punctuation">.</span>memcpy_htod_async<span class="token punctuation">(</span>inp<span class="token punctuation">[</span><span class="token string">'device'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> inp<span class="token punctuation">[</span><span class="token string">'host'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> stream<span class="token punctuation">)</span> <span class="token keyword">for</span> inp <span class="token keyword">in</span> inputs<span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># trt context</span>    context<span class="token punctuation">.</span>execute_async_v2<span class="token punctuation">(</span>bindings<span class="token operator">=</span>bindings<span class="token punctuation">,</span> stream_handle<span class="token operator">=</span>stream<span class="token punctuation">.</span>handle<span class="token punctuation">)</span>    <span class="token punctuation">[</span>cuda<span class="token punctuation">.</span>memcpy_dtoh_async<span class="token punctuation">(</span>out<span class="token punctuation">[</span><span class="token string">'host'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> out<span class="token punctuation">[</span><span class="token string">'device'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> stream<span class="token punctuation">)</span> <span class="token keyword">for</span> out <span class="token keyword">in</span> outputs<span class="token punctuation">]</span>    stream<span class="token punctuation">.</span>synchronize<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>cuda也有context的概念，主要负责gpu运算以及gpu和cpu之间的数据交换。</p><p>新建cuda context的花销比较大，所以在后端要使用pop和push方法隐藏和激活</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> pycuda<span class="token punctuation">.</span>driver <span class="token keyword">as</span> cudactx <span class="token operator">=</span> cuda<span class="token punctuation">.</span>Device<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>make_context<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 调用pop和push方法隐藏和激活</span>cuda<span class="token punctuation">.</span>push<span class="token punctuation">(</span><span class="token punctuation">)</span>cuda<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="五、最后"><a href="#五、最后" class="headerlink" title="五、最后"></a>五、最后</h1><p>测试tensorrt花销为20ms，正常gpu速度为30ms，提升了50%。如果是一些v100或者a100这样的专业显卡的话提升会更多。</p>]]></content>
      
      
      <categories>
          
          <category> 深度实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法拾贝 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>混合精度，用好loss scale让pytorch凌波微步</title>
      <link href="/2021/08/03/hun-he-jing-du-xun-lian-shi-zhan/"/>
      <url>/2021/08/03/hun-he-jing-du-xun-lian-shi-zhan/</url>
      
        <content type="html"><![CDATA[<p>书到用时方恨少，训练慢时恨卡少。不过混合精度训练可以带来30%以上的up体验<a href="https://arxiv.org/pdf/1710.03740.pdf" target="_blank" rel="noopener">(Mixed Precison Training)</a></p><h1 id="一、apex用法"><a href="#一、apex用法" class="headerlink" title="一、apex用法"></a>一、apex用法</h1><ul><li>O0:单精度fp32  </li><li>O3:半精度fp16  </li><li>O1:mix precision(<strong>recommand</strong>)  </li><li>O2:almost fp16  </li></ul><h1 id="二、torch-1-6内置用法"><a href="#二、torch-1-6内置用法" class="headerlink" title="二、torch 1.6内置用法"></a>二、torch 1.6内置用法</h1><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>amp <span class="token keyword">import</span> GradScaler<span class="token punctuation">,</span>autocastscaler <span class="token operator">=</span> GradScaler<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">with</span> autocast<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    output <span class="token operator">=</span> model<span class="token punctuation">(</span>input<span class="token punctuation">)</span>    loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>scaler<span class="token punctuation">.</span>scale<span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>scaler<span class="token punctuation">.</span>step<span class="token punctuation">(</span>optimizer<span class="token punctuation">)</span>scaler<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="三、主要步骤"><a href="#三、主要步骤" class="headerlink" title="三、主要步骤"></a>三、主要步骤</h1><ul><li>Mix Precision  <ul><li>根据预设的黑白名单，对算子选择是否混合精度计算：乘法计算采用fp16，加法累加采用fp32<br><img src="/resources/amp%E5%8D%95%E5%85%83.png" alt>  </li></ul></li><li>Loss Scaling  <ul><li>在计算loss时适当放大loss，在优化器更新参数时缩小同样倍数梯度。目前apex支持动态放缩倍数。  </li></ul></li></ul><h1 id="四、loss-scale的思想"><a href="#四、loss-scale的思想" class="headerlink" title="四、loss scale的思想"></a>四、loss scale的思想</h1><p>在交易系统中算钱的时候，规范的做法是把金额如1.01元*100之后再做计算，计算完之后再除以100，这样可以避免0.01无法用二进制精确表示造成的舍入误差</p><h2 id="怎么用好loss-scale"><a href="#怎么用好loss-scale" class="headerlink" title="怎么用好loss scale"></a>怎么用好loss scale</h2><h3 id="理想情况-来自官方文档"><a href="#理想情况-来自官方文档" class="headerlink" title="理想情况(来自官方文档)"></a>理想情况(来自官方文档)</h3><p><img src="/resources/loss_scale.png" alt>  </p><h3 id="现实-动态loss-scale梯度爆炸"><a href="#现实-动态loss-scale梯度爆炸" class="headerlink" title="现实(动态loss scale梯度爆炸)"></a>现实(动态loss scale梯度爆炸)</h3><pre class="line-numbers language-text"><code class="language-text">Gradient overflow. Skipping step, loss scaler 0 reducing loss scale to 32768.0Gradient overflow. Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow. Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow. Skipping step, loss scaler 0 reducing loss scale to 4096.0Gradient overflow. Skipping step, loss scaler 0 reducing loss scale to 2048.0...Gradient overflow. Skipping step, loss scaler 0 reducing loss scale to 1.0...Gradient overflow. Skipping step, loss scaler 0 reducing loss scale to 0.00048828125Gradient overflow. Skipping step, loss scaler 0 reducing loss scale to 0.000244140625Gradient overflow. Skipping step, loss scaler 0 reducing loss scale to 0.0001220703125Gradient overflow. Skipping step, loss scaler 0 reducing loss scale to 6.103515625e-05Gradient overflow. Skipping step, loss scaler 0 reducing loss scale to 3.0517578125e-05<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="五、踩坑心得"><a href="#五、踩坑心得" class="headerlink" title="五、踩坑心得"></a>五、踩坑心得</h1><ol><li>可以不用loss scale，直接走loss.backward(),根据收敛情况调节学习率,实测bert微调ner时学习率需要调大一点（建议10倍往上调），不过确实精度会差几个点 <ul><li>论文原话:We trained a variety of networks with scaling factors ranging from 8 to 32K(many networks did not require a scaling factor)</li></ul></li><li>loss scale时梯度偶尔overflow可以忽略，因为amp会检测溢出情况并跳过该次更新（如果自定义了optimizer.step的返回值，会发现溢出时step返回值永远是None），scaler下次会自动缩减倍率，如果长时间稳定更新，scaler又会尝试放大倍数  </li><li>一直显示overflow而且loss很不稳定的话就需要适当调小学习率（建议10倍往下调），如果loss还是一直在波动，那可能是网络深层问题了。</li></ol><h1 id="六、实验数据"><a href="#六、实验数据" class="headerlink" title="六、实验数据"></a>六、实验数据</h1><p>bert微调ner的小实验，相同batch size和epoch，在挑战集评测结果  </p><table><thead><tr><th align="left">name</th><th align="left">初始学习率</th><th align="left">f1</th><th align="center">p</th><th align="left">r</th><th align="left">显存占用</th><th align="left">相对速度</th></tr></thead><tbody><tr><td align="left">fp32</td><td align="left">1e-5</td><td align="left">0.991</td><td align="center">0.992</td><td align="left">0.989</td><td align="left">9097M</td><td align="left">5.4</td></tr><tr><td align="left">amp no scale</td><td align="left">10e-5</td><td align="left">0.953</td><td align="center">0.963</td><td align="left">0.943</td><td align="left">7523M</td><td align="left">7.0</td></tr><tr><td align="left">amp dynamic scale</td><td align="left">0.1e-5</td><td align="left">0.962</td><td align="center">0.968</td><td align="left">0.955</td><td align="left">7523M</td><td align="left">7.0</td></tr></tbody></table><h2 id="tensorflow开启混合精度-nvidia开发者文档"><a href="#tensorflow开启混合精度-nvidia开发者文档" class="headerlink" title="tensorflow开启混合精度(nvidia开发者文档)"></a>tensorflow开启混合精度<a href="https://developer.nvidia.com/automatic-mixed-precision" target="_blank" rel="noopener">(nvidia开发者文档)</a></h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> osos<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'TF_ENABLE_AUTO_MIXED_PRECISION'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'1'</span>optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span>enable_mixed_precision_graph_rewrite<span class="token punctuation">(</span>optimizer<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法拾贝 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人生最重要的三十年</title>
      <link href="/2020/10/30/ren-sheng-zui-chong-yao-de-san-shi-nian/"/>
      <url>/2020/10/30/ren-sheng-zui-chong-yao-de-san-shi-nian/</url>
      
        <content type="html"><![CDATA[<p>作为一个25岁还在为未来发愁的职前青年，看到一片博文深有感触，职业生涯也许就是在这黄金三十年里面，简单的梳理了一下思路：</p><p>第一个十年是积累的十年。</p><p>最开始的起点很重要，尽量选择一个可以看清前景的行业，当然如果是行业食物链中越往上越好。</p><p>没有无前途的行业，只有没前途的企业与个人。</p><p>从职涯与人生看，这十年要做到基本的三点。</p><p>一是学会最基本的技能，比如听话、做事、沟通的基本技能，比如时间管理，比如安排业余生活与工作的平衡等生命中最基本的技能。这十年是修身培养品性的十年，如不成则基本不成，如小成则人生小成，如大成则未来的人生终究会走向辉煌。说好话，做好事，做好人。（保持工作与生活的平衡至关重要）</p><p>二是建立一个美满的家庭，维系一个好的人脉圈，这是人生幸福的来源。所有财富，所有成就都是过眼云烟，一个人的人生幸福最终是映射在他身边最重要的人身上。（寻求可以交付一生的爱情）</p><p>三是积累一定的财富，比如有两至三处房产，学会理财与投资。第三点没有第一二点重要。但由于这十年结婚生子购房等大事均在其间发生，所以财富积累的速度非常重要。（很实在，但又无法回避）</p><p>做到以上三点可以进一步思考自己的主业。</p><p>理想的人生都应当有一个主业，自己会用心去做的而且对世界来说美好的事情，但相信世界上百分之九十的人最终没有找到他的主业。</p><p>第一个十年不一定与主业密切相关，但关键是尽早发现你的主业，尽早开始准备。</p><p>第一个十年的职涯上一定要成为一行业的专家，这是你财富的根源，创业与打工都一样。</p><p>第二个十年是三十五到四十五的十年，这十年最重要的三个目标，</p><p>一是好好的培养子女，这十年是子女成长最重要的十年，这是有子女的朋友这十年最重要的目标了。其实也是唯一的子女真正我们可陪伴的十年。</p><p>二是开始从事自己的主业了，这时的主业不一定是职业，但这十年必须有勇气与决心开始自己主业了，这是上苍给我们天赐才能要珍惜与发挥的时候了。</p><p>主业不一定是创业，但做主业对于人生而言很多时候是人生的创业。</p><p>三是在财富积累与职业发展的基础上发展自己的德行，将财产一定比例的投入公益，将时间的一部分投入社会事业。</p><p>退休不是人生的目标，发挥出主业，并为世界创造更多美好与德行才是必须的。</p><p>第三个十年是四十五到五十五，这是职业生涯末尾的十年。这十年的目标。</p><p>一是将主业变成职业即事业，将最黄金的十年，投入到毕生最重要的事业上去，让它开花结果。</p><p>二是完成退休后的准备，黄金十年也许是在顶峰。但顶峰之后毕竟不管何时终究走向黄昏。</p><p>下山的路如何行走，或是将下山变成另一段旅程，是这十年的课业。</p><p>注记：我现在正面临职业的抉择，计算机是目前最适合我的一个方向，不论是兴趣方面还是技术方面还有行业前景上都是符合自己的期望的，所以既然选择了，就要一往无前的走下去。</p><p>注记：写在28岁，现在在一家医健行业的信息技术公司从事自然语言处理，为了自己的未来工作</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
