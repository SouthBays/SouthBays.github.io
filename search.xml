<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>三十而立</title>
      <link href="/2022/02/26/san-shi-er-li/"/>
      <url>/2022/02/26/san-shi-er-li/</url>
      
        <content type="html"><![CDATA[<p>如果有一种人生叫成功，那一定是能按照自己的计划去铺展未来。</p><p>我的人生，就是能够选择自己擅长的事情，一步步积累成就感。</p><blockquote><p>『无题』 <sub><sup>风の诗</sup></sub><br>人闻醉酒多诗仙，<br>唯有醒者真圣贤。<br>梦里有之不必有，<br>梦里无之倏忽见。<br>白驹过隙一页黄，<br>沧海闻涛往事烟。<br>此去灵山多有路，<br>独不见不知所然。  </p></blockquote><p>诗与酒，酒是麻痹神经的，诗可以遨游太虚而不需要任何代价，何不如清醒地诗意人生？</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
          <category> 诗歌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>句子嵌入向量的简单对比学习方法</title>
      <link href="/2022/02/10/ju-zi-qian-ru-xiang-liang-de-jian-dan-dui-bi-xue-xi-fang-fa/"/>
      <url>/2022/02/10/ju-zi-qian-ru-xiang-liang-de-jian-dan-dui-bi-xue-xi-fang-fa/</url>
      
        <content type="html"><![CDATA[<h1 id="pretraining-model-constractive-learning-SoTA-sentence-embedding！"><a href="#pretraining-model-constractive-learning-SoTA-sentence-embedding！" class="headerlink" title="pretraining model + constractive learning=SoTA sentence embedding！  "></a>pretraining model + constractive learning=SoTA sentence embedding！  </h1><p><a href="https://arxiv.org/pdf/2104.08821.pdf" target="_blank" rel="noopener">(PaperOrigin)</a>  <a href="./对比学习/2104.08821.pdf">(PaperLocal)</a>  </p><ol><li>Contracdiction pairs of NLI(自然语言推理)<br> 先看nli推理是将句子对输出两次句向量，并计算其相似度。<br> <img src="/resources/nlinet.png" alt>  </li></ol><ol start="2"><li><p>Pretraining model(预训练模型)<br> 再看一个利用bert的[CLS]标签编码两两作点积来训练相似度(主要看左侧矩阵，来自苏剑林的simbert)。<br> <img src="/resources/simbert.png" alt>  </p><blockquote><p>第一个方法是构造相似或者不向似的句子对，预测0和1的标签，是一个有监督学习<br>第二个方法可以用于无监督学习。在一个batch的n个句子中，自己作为正例，除了自己以外的其它句子都作为负例，最后生成一个n<em>n的相似度矩阵，预测的目标就是一个n</em>n的单位矩阵  </p></blockquote><p> SimCSE就是融合了两种方法，它的正例是对自己重复编码dropout生成的不同向量(后面会具体解释)，负例是是其它句子重复编码生成的向量，同样生成一个n*n的相似度矩阵。<br> <img src="/resources/simcse.jpg" alt>  </p></li></ol><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><ol><li><p>info NCE loss<br><img src="/resources/infonce_loss.jpg" alt>  </p></li><li><p>stand dropout vs other data augmentation</p><ul><li><code>SimCSE (dropout)</code> NLP is interesting. —— NLP is interesting.</li><li><code>Next sentence</code> I do NLP. —— NLP is interesting.  </li><li><code>Synonym replacement</code> The movie is <u>great</u>. —— The movie is <u>fantastic</u>.  </li><li><code>crops</code> <del>Two dogs</del> are running. —— <del>Two</del> dogs are <del>running</del>.  </li><li><code>Delete one word</code> Two dogs are <del>running</del>. —— Two dogs <del>are</del> running.</li></ul></li></ol><h2 id="深层原因"><a href="#深层原因" class="headerlink" title="深层原因?"></a>深层原因?</h2><ol><li><p><strong>alignment</strong> vs. <strong>uniformity</strong><a href="https://arxiv.org/pdf/2005.10242.pdf" target="_blank" rel="noopener">(PaperOrigin)</a>  <a href="./对比学习/2005.10242.pdf">(PaperLocal)</a><br> <img src="/resources/alignment_uniformity.jpg" alt>   </p><blockquote><p>alignment:正样本的对齐性<br>uniformity:所有样本分布的一致性  </p></blockquote></li><li><p>对SimCSE等方法定量分析训练过程中的对齐性和一致性<br> <img src="/resources/simcse_deepdive.jpg" alt>  </p><blockquote><p>SimCSE表现最好，在提升一致性的同时很好的保持了对齐性  </p></blockquote></li></ol><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><ol><li>bert的原始的[CLS]向量不可靠  </li><li>前置研究句向量的降维-bert whitening也印证了[CLS]向量包含较多的冗余信息，当然，whitening最主要的作用还是将句向量转化到标准正交基上，从而更好地使用余弦相似度<br> <a href="https://arxiv.org/abs/2103.15316" target="_blank" rel="noopener">(PaperOrigin)</a>  <a href="./对比学习/2103.15316.pdf">(PaperLocal)</a><br> <a href="https://spaces.ac.cn/archives/8069" target="_blank" rel="noopener">博客地址</a><br> 一个线性变换-PCA降维可以把维度从768降到256,1024降到384而保持差不多的结果。  </li><li>SimCSE可以处理的文本更长，因为句子对是分别编码的，而降维思想可以在储存和检索向量时大大减少空间和内存的占用从而提升检索效率  <ul><li>应用场景1. 实体链接  </li><li>应用场景2. 文本聚类  </li><li>应用场景3. 问句检索  </li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 论文学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法拾贝 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人生最重要的三十年</title>
      <link href="/2020/10/30/ren-sheng-zui-chong-yao-de-san-shi-nian/"/>
      <url>/2020/10/30/ren-sheng-zui-chong-yao-de-san-shi-nian/</url>
      
        <content type="html"><![CDATA[<p>作为一个25岁还在为未来发愁的职前青年，看到一片博文深有感触，职业生涯也许就是在这黄金三十年里面，简单的梳理了一下思路：</p><p>第一个十年是积累的十年。</p><p>最开始的起点很重要，尽量选择一个可以看清前景的行业，当然如果是行业食物链中越往上越好。</p><p>没有无前途的行业，只有没前途的企业与个人。</p><p>从职涯与人生看，这十年要做到基本的三点。</p><p>一是学会最基本的技能，比如听话、做事、沟通的基本技能，比如时间管理，比如安排业余生活与工作的平衡等生命中最基本的技能。这十年是修身培养品性的十年，如不成则基本不成，如小成则人生小成，如大成则未来的人生终究会走向辉煌。说好话，做好事，做好人。（保持工作与生活的平衡至关重要）</p><p>二是建立一个美满的家庭，维系一个好的人脉圈，这是人生幸福的来源。所有财富，所有成就都是过眼云烟，一个人的人生幸福最终是映射在他身边最重要的人身上。（寻求可以交付一生的爱情）</p><p>三是积累一定的财富，比如有两至三处房产，学会理财与投资。第三点没有第一二点重要。但由于这十年结婚生子购房等大事均在其间发生，所以财富积累的速度非常重要。（很实在，但又无法回避）</p><p>做到以上三点可以进一步思考自己的主业。</p><p>理想的人生都应当有一个主业，自己会用心去做的而且对世界来说美好的事情，但相信世界上百分之九十的人最终没有找到他的主业。</p><p>第一个十年不一定与主业密切相关，但关键是尽早发现你的主业，尽早开始准备。</p><p>第一个十年的职涯上一定要成为一行业的专家，这是你财富的根源，创业与打工都一样。</p><p>第二个十年是三十五到四十五的十年，这十年最重要的三个目标，</p><p>一是好好的培养子女，这十年是子女成长最重要的十年，这是有子女的朋友这十年最重要的目标了。其实也是唯一的子女真正我们可陪伴的十年。</p><p>二是开始从事自己的主业了，这时的主业不一定是职业，但这十年必须有勇气与决心开始自己主业了，这是上苍给我们天赐才能要珍惜与发挥的时候了。</p><p>主业不一定是创业，但做主业对于人生而言很多时候是人生的创业。</p><p>三是在财富积累与职业发展的基础上发展自己的德行，将财产一定比例的投入公益，将时间的一部分投入社会事业。</p><p>退休不是人生的目标，发挥出主业，并为世界创造更多美好与德行才是必须的。</p><p>第三个十年是四十五到五十五，这是职业生涯末尾的十年。这十年的目标。</p><p>一是将主业变成职业即事业，将最黄金的十年，投入到毕生最重要的事业上去，让它开花结果。</p><p>二是完成退休后的准备，黄金十年也许是在顶峰。但顶峰之后毕竟不管何时终究走向黄昏。</p><p>下山的路如何行走，或是将下山变成另一段旅程，是这十年的课业。</p><p>注记：我现在正面临职业的抉择，计算机是目前最适合我的一个方向，不论是兴趣方面还是技术方面还有行业前景上都是符合自己的期望的，所以既然选择了，就要一往无前的走下去。</p><p>注记：写在28岁，现在在一家医健行业的信息技术公司从事自然语言处理，为了自己的未来工作</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
